{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e5a4f02-12ef-4e6b-a86a-38fb0274c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d1ce60-e529-4595-83e4-46e0b51dc7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are advanced AI systems that can understand, generate, and comprehend human language. Here's a simple breakdown:\n",
      "\n",
      "1. **What They Are**:  \n",
      "   LLMs, like GPT-3 or ChatGPT, are computer programs designed to analyze text data and produce responses based on what they've learned.\n",
      "\n",
      "2. **How They Work**:  \n",
      "   - They're trained on vast amounts of text (like books, websites, etc.) to understand patterns in language.\n",
      "   - When given a query, they use this understanding to generate answers or create new text.\n",
      "\n",
      "3. **Key Features**:\n",
      "   - **Natural Language Processing**: They can understand and produce human-like text.\n",
      "   - **Versatility**: They handle various tasks like writing, answering questions, and even creative writing.\n",
      "   - **Zero-shot Learning**: They can perform tasks they haven't been explicitly trained on by using general knowledge.\n",
      "\n",
      "4. **Applications**:\n",
      "   - **Text Generation**: Writing articles, stories, or emails.\n",
      "   - **问答 (Q&A)**: Answering questions across many topics.\n",
      "   - **Translation**: Translating text from one language to another.\n",
      "   - **Content Creation**: Assisting in creating content for websites, marketing, etc.\n",
      "\n",
      "5. **Limitations**:\n",
      "   - They understand words based on their context and usage in the training data.\n",
      "   - They don't have real-life experience or consciousness, so their responses are based solely on patterns they've learned.\n",
      "   - Overly formal or repetitive outputs might occur if not used carefully.\n",
      "\n",
      "6. **Impact**:  \n",
      "   LLMs are transforming industries by automating content creation, providing insights, and helping with information retrieval.\n",
      "\n",
      "In summary, LLMs are sophisticated AI systems that mimic human language to perform a wide range of tasks, but they operate based on the data they've been trained on without any understanding of real-world context or personal experience.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='deepseek-r1:7b',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain LLMs in simple terms\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e43b588d-8709-429a-bc9e-f998edd2c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are advanced AI systems that can understand and generate human language. They're trained on vast amounts of text data to recognize patterns and answer questions creatively. Think of them as super-smart assistants that can converse naturally, write essays, solve problems, or even assist in creative tasks.\n",
      "\n",
      "Here's a simple breakdown:\n",
      "\n",
      "1. **Understanding**: LLMs can comprehend complex texts, articles, books, and other written materials.\n",
      "2. **Generating Text**: They can produce human-like text on any topic, including writing stories, creating content, or providing explanations.\n",
      "3. **Learning**: Through training, LLMs improve over time, getting better at understanding context and generating more relevant responses.\n",
      "4. **Versatility**: They work across various domains like law, science, art, and more, adapting to different types of tasks.\n",
      "\n",
      "Examples include GPT-3.5 and ChatGPT, which are widely used for their ability to assist in diverse ways, from answering questions to creative writing."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='deepseek-r1:7b',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain LLMs in simple terms\"\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream token by token\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48feb6fd-bfe9-42fe-b1a3-a2631e8847e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='deepseek-r1:7b',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a technical explanation of transformers\"\n",
    "        }\n",
    "    ]\n",
    "    options={ \n",
    "        \"temprature\" : 0.2,\n",
    "        \"top_p\" : 0.9, \n",
    "        \"num_ctx\" : 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d15905-9e12-40a5-ab07-ec0361b40b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ollama.chat(\n",
    "    model='deepseek-r1:7b',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are a cybersecurity AI expert\"\n",
    "        }\n",
    "        {\n",
    "        \"role\" : \"system\",\n",
    "        \"content\" : \"Explain malware behavioue analysis\"    \n",
    "    ]\n",
    ")\n",
    "#This directly alters model conditioning vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1d670-38f5-4f7f-be4a-fdd0cc7dc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text,chunk_size=500, overlaps=50):\n",
    "    chunks =[]\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlaps\n",
    "\n",
    "    return chunks     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47102055-9bdf-4b58-b1a7-e63476fbe6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = ollama.embeddings(\n",
    "        model='nomic-embed-text',\n",
    "        prompt=text\n",
    "    )\n",
    "    return np.array(response['embedding'], dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f999934-cd77-4a4a-b798-544d58c50f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c086fbe-d7f3-416c-88b7-b9a298a956bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, dim):\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.texts = []\n",
    "\n",
    "    def add(self, embeddings, texts):\n",
    "        self.index.add(np.array(embeddings, dtype='float32'))\n",
    "        self.texts.extend(texts)\n",
    "\n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding], dtype='float32'),\n",
    "            top_k\n",
    "        )\n",
    "        results = [self.texts[i] for i in indices[0]]\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3b802-bbd1-4fd5-911b-82d04763607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, store, top_k=3):\n",
    "    query embedding = get_embedding(query)\n",
    "    return store.search(query_embedding, top_k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e86a05-fe18-4c5e-8af9-6d3c95bc85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, contexts):\n",
    "    context_block = \"\\n\\n\".join(contexts)\n",
    "    return f\"\"\"\n",
    "\n",
    "    You are an expert assistant. Answer strictly using the context below.\n",
    "    If the answer is not in the context, say \"Not found in provided documents.\"\n",
    "\n",
    "    context:\n",
    "    {context_block}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55e0f0-9669-4f7e-b9f3-5c08b7d1e1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00639196-827c-4be9-a7c6-17e97537f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, dim):\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.texts = []\n",
    "\n",
    "    def add(self, embeddings, texts):\n",
    "        self.index.add(np.array(embeddings, dtype='float32'))\n",
    "        self.texts.extend(texts)\n",
    "\n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding], dtype='float32'),\n",
    "            top_k\n",
    "        )\n",
    "        results = [self.texts[i] for i in indices[0]]\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dff41a4-8754-43a2-b6d3-9a9a0f73807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    response = ollama.chat(\n",
    "        model='deepseek-r1:7b',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9a60d-fb74-4cdc-b064-48d3617b5085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
